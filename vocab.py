import json
import pickle
import re
import os
import nltk
from collections import Counter
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
CAPTION_DIR = Path("./data/captions")
TRAIN_JSON = CAPTION_DIR / "train_3.json"  # ì˜¤ì§ train ë°ì´í„°ë§Œ ì‚¬ìš©
SAVE_PATH = "vocab_3.pkl"

NLTK_DATA_PATH = "./nltk_data"
if not os.path.exists(NLTK_DATA_PATH):
    os.makedirs(NLTK_DATA_PATH)
nltk.data.path.insert(0, NLTK_DATA_PATH)

for resource in ['punkt', 'punkt_tab']:
    try:
        nltk.data.find(f'tokenizers/{resource}', paths=[NLTK_DATA_PATH])
    except LookupError:
        nltk.download(resource, download_dir=NLTK_DATA_PATH)

class Vocabulary:
    def __init__(self, freq_threshold=1): # <UNK> ê°ì†Œë¥¼ ìœ„í•´ 1ë¡œ í•˜í–¥ ì¡°ì •
        self.itos = {0: "<PAD>", 1: "<START>", 2: "<END>", 3: "<UNK>"}
        self.stoi = {"<PAD>": 0, "<START>": 1, "<END>": 2, "<UNK>": 3}
        self.freq_threshold = freq_threshold

    def __len__(self):
        return len(self.itos)

    @staticmethod
    def tokenizer(text):
        # 1. ì†Œë¬¸ìí™” ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±° (ì •ê·œí‘œí˜„ì‹)
        # ì•ŒíŒŒë²³ê³¼ ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text.lower())
        return nltk.word_tokenize(text)

    def build_vocabulary(self, sentence_list):
        frequencies = Counter()
        idx = 4 

        print(f"[*] {len(sentence_list)}ê°œì˜ Train ë¬¸ì¥ìœ¼ë¡œë¶€í„° ë‹¨ì–´ ìˆ˜ì§‘ ì¤‘...")
        for sentence in sentence_list:
            for word in self.tokenizer(sentence):
                frequencies[word] += 1

                if frequencies[word] == self.freq_threshold:
                    if word not in self.stoi:
                        self.stoi[word] = idx
                        self.itos[idx] = word
                        idx += 1
        
        # ë°ì´í„° ì¹´ë“œìš© í†µê³„ ì¶œë ¥
        self.display_stats(frequencies)

    def display_stats(self, frequencies):
        """ê¸°ìˆ  ë³´ê³ ì„œ ë° ë°ì´í„° ì¹´ë“œìš© í†µê³„ ì‹œê°í™” ë³´ì¡°"""
        print("-" * 40)
        print(f"ğŸ“Š Vocabulary Statistics")
        print(f"  - Total Vocab Size (including special tokens): {len(self)}")
        print(f"  - Total Unique Tokens found: {len(frequencies)}")
        print(f"  - Tokens kept (freq >= {self.freq_threshold}): {len(self.stoi) - 4}")
        print(f"  - Top 5 Common Words: {frequencies.most_common(5)}")
        print("-" * 40)

    def numericalize(self, text):
        tokenized_text = self.tokenizer(text)
        return [self.stoi.get(token, self.stoi["<UNK>"]) for token in tokenized_text]

def save_vocab(vocab, path):
    with open(path, 'wb') as f:
        pickle.dump(vocab, f)
    print(f"[SUCCESS] {path} ì €ì¥ ì™„ë£Œ.")

if __name__ == "__main__":
    if not TRAIN_JSON.exists():
        print(f"[ERROR] {TRAIN_JSON} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        with open(TRAIN_JSON, 'r', encoding='utf-8') as f:
            data = json.load(f)
            train_captions = [item['caption'] for item in data]

        # freq_thresholdë¥¼ 1ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  train ë‹¨ì–´ ìˆ˜ìš©
        vocab = Vocabulary(freq_threshold=1)
        vocab.build_vocabulary(train_captions)
        save_vocab(vocab, SAVE_PATH)
        
        print(f"[*] ìµœì¢… êµ¬ì¶•ëœ ë‹¨ì–´ ì‚¬ì „ í¬ê¸°(vocab_size): {len(vocab)}")
        # í…ŒìŠ¤íŠ¸
        sample = "A red flower, in the garden!"
        print(f"\n[TEST] ì›ë¬¸: {sample}")
        print(f"[TEST] ì •ì œ í›„ í† í°í™”: {vocab.tokenizer(sample)}")