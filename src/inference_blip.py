import torch
import torchvision.transforms as transforms
from PIL import Image
import pickle
import os
import sys
import glob
import random
import matplotlib.pyplot as plt
from deep_translator import GoogleTranslator
import re
import nltk

# ==========================================
# 1. â˜… í•µì‹¬ ìˆ˜ì •: Vocabulary í´ë˜ìŠ¤ ì§ì ‘ ì •ì˜ â˜…
# ==========================================
# vocab_3.pkl íŒŒì¼ì´ "ì´ í´ë˜ìŠ¤ê°€ ë©”ì¸ í™”ë©´ì— ìˆì–´ì•¼ í•´!"ë¼ê³  ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
class Vocabulary:
    def __init__(self, freq_threshold=1):
        self.itos = {0: "<PAD>", 1: "<START>", 2: "<END>", 3: "<UNK>"}
        self.stoi = {"<PAD>": 0, "<START>": 1, "<END>": 2, "<UNK>": 3}
        self.freq_threshold = freq_threshold

    def __len__(self):
        return len(self.itos)

    @staticmethod
    def tokenizer(text):
        # ì •ê·œì‹ìœ¼ë¡œ íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ í† í°í™”
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text.lower())
        return nltk.word_tokenize(text)

    def numericalize(self, text):
        tokenized_text = self.tokenizer(text)
        return [self.stoi.get(token, self.stoi["<UNK>"]) for token in tokenized_text]

# ==========================================
# 2. ì„¤ì •
# ==========================================
JIN_FOLDER = "."   # í˜„ì¬ í´ë”
IMAGE_FOLDER = "train"  # ì´ë¯¸ì§€ í´ë”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í† í¬ë‚˜ì´ì € ì—ëŸ¬ ë°©ì§€)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

# ==========================================
# 3. íŒŒì¼ ë° ëª¨ë“ˆ ë¡œë“œ
# ==========================================
vocab_files = glob.glob("*.pkl")
model_files = glob.glob("*.pth")

if not vocab_files or not model_files:
    print("ğŸš¨ [ì—ëŸ¬] .pkl ë˜ëŠ” .pth íŒŒì¼ì´ í˜„ì¬ í´ë”ì— ì—†ìŠµë‹ˆë‹¤.")
    sys.exit()

VOCAB_PATH = vocab_files[0]
MODEL_PATH = model_files[0]
print(f"âœ… íŒŒì¼ ë¡œë“œ: {VOCAB_PATH}, {MODEL_PATH}")

# model.py ë¶ˆëŸ¬ì˜¤ê¸°
try:
    from model import CNNtoRNN 
except ImportError:
    print("ğŸš¨ [ì—ëŸ¬] 'model.py' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ í´ë”ì— ë³µì‚¬í•´ì£¼ì„¸ìš”!")
    sys.exit()

# ==========================================
# 4. ê¸°ëŠ¥ í•¨ìˆ˜ (í›„ê°€ê³µ & ìƒì„±)
# ==========================================
def process_multimodal_output(raw_caption):
    text = raw_caption.replace('<start>', '').replace('<end>', '').replace('<pad>', '').strip()
    NOISE_KEYWORDS = ['melon', 'professional', 'photo', 'stock', 'image', 'high', 'resolution', 'vector', 'illustration']
    STOP_WORDS = ['and', 'with', 'a', 'the', 'in', 'on', 'at', 'of']
    
    if '.' in text: text = text.split('.')[0].strip()
    words = text.split()
    cutoff = len(words)
    for i, w in enumerate(words):
        if "".join(filter(str.isalnum, w.lower())) in NOISE_KEYWORDS:
            cutoff = i; break
    words = words[:cutoff]
    while words and words[-1].lower() in STOP_WORDS: words.pop()
    
    final_eng = " ".join(words) + "." if words else text + "."
    try: final_kor = GoogleTranslator(source='auto', target='ko').translate(final_eng)
    except: final_kor = "ë²ˆì—­ ì‹¤íŒ¨"
    return final_eng, final_kor

def generate_caption_step_by_step(model, image, vocab, max_len=20):
    result_caption = []
    with torch.no_grad():
        features = model.encoderCNN(image).unsqueeze(1)
        states = None 
        _, states = model.decoderRNN.rnn(features, states)
        
        # <START> í† í°ìœ¼ë¡œ ì‹œì‘
        start_token = vocab.stoi.get("<START>", 1)
        inputs = model.decoderRNN.embed(torch.tensor([start_token]).to(device)).unsqueeze(1)
        
        for _ in range(max_len):
            hiddens, states = model.decoderRNN.rnn(inputs, states)
            outputs = model.decoderRNN.linear(hiddens.squeeze(1))
            predicted = outputs.argmax(1)
            word = vocab.itos[predicted.item()]
            
            if word == "<END>": break
            result_caption.append(word)
            inputs = model.decoderRNN.embed(predicted).unsqueeze(1)
            
    return " ".join(result_caption)

# ==========================================
# 5. ë©”ì¸ ì‹¤í–‰
# ==========================================
if __name__ == "__main__":
    # 1. Vocab ë¡œë“œ (ì´ì œ ì—ëŸ¬ ì•ˆ ë‚¨!)
    with open(VOCAB_PATH, 'rb') as f:
        vocab = pickle.load(f)
    print(f"ğŸ“– ë‹¨ì–´ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ (í¬ê¸°: {len(vocab)})")

    # 2. ëª¨ë¸ ì„¤ì • ë¡œë“œ
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    
    # ì„¤ì •ê°’ (ê¸°ë³¸ê°’ + ì €ì¥ëœ ê°’)
    cfg = checkpoint.get('config', {})
    embed_size = cfg.get('embed_size', 300)
    hidden_size = cfg.get('hidden_size', 512)
    num_layers = cfg.get('num_layers', 1)
    encoder_type = cfg.get('encoder_type', 'mobilenet_v2')
    decoder_type = cfg.get('decoder_type', 'lstm')

    # 3. ëª¨ë¸ ìƒì„± & ê°€ì¤‘ì¹˜ ì ìš©
    model = CNNtoRNN(embed_size, hidden_size, len(vocab), num_layers, encoder_type, decoder_type).to(device)
    model.load_state_dict(checkpoint.get('model_state_dict', checkpoint))
    model.eval()

    # 4. ì´ë¯¸ì§€ ì¶”ë¡ 
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
    ])
    
    img_files = glob.glob(os.path.join(IMAGE_FOLDER, "**/*.jpg")) + glob.glob(os.path.join(IMAGE_FOLDER, "**/*.png"))
    
    if img_files:
        samples = random.sample(img_files, min(20, len(img_files))) # 20ê°œ í™•ì¸
        
        plt.figure(figsize=(20, 10))
        # 5ì—´ ê·¸ë¦¬ë“œ ê³„ì‚°
        cols = 5
        rows = (len(samples) + cols - 1) // cols
        
        print("\nğŸš€ ê²°ê³¼ ìƒì„± ì¤‘...\n")
        for i, img_path in enumerate(samples):
            try:
                image = Image.open(img_path).convert('RGB')
                img_tensor = transform(image).unsqueeze(0).to(device)
                
                raw = generate_caption_step_by_step(model, img_tensor, vocab)
                eng, kor = process_multimodal_output(raw)
                
                plt.subplot(rows, cols, i+1)
                plt.imshow(image)
                plt.axis("off")
                
                # í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ
                wrapped_text = f"[Eng] {eng[:40]}\n[Kor] {kor[:30]}"
                plt.title(wrapped_text, fontsize=9, loc='left', color='blue')
                print(f"[{i+1}] {eng}")
                
            except Exception as e:
                print(f"ì—ëŸ¬: {e}")
                
        plt.tight_layout()
        plt.show()
    else:
        print("ğŸš¨ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")